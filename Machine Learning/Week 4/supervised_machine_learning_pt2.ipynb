{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "234ea51e-1a49-40cf-a2ac-69ae70d51281",
   "metadata": {},
   "source": [
    "<h1 style='font-size:40px'>Supervised Machine Learning - Part 2</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f245340d-5e1f-4ea0-aa5d-11c77fa0e12a",
   "metadata": {},
   "source": [
    "<h2 style='font-size:30px'> Naive Bayes Classifiers</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51b7b6fd-d40c-48a5-8b34-82b9c14d3fdd",
   "metadata": {},
   "source": [
    "<div>\n",
    "    <ul style='font-size:20px'>\n",
    "        <li>\n",
    "            O Naïve Bayes é um algoritmo cuja premissa é que as diferentes features das instâncias do dataset não são correlacionadas (por isso diz-se que ele é ingênuo).\n",
    "        </li>\n",
    "        <li>\n",
    "            Ele é bastante eficiente em seu processo de aprendizado e tem uma alta performance em algumas tarefas, mas sua suposição inicial pode levá-lo a ter piores resultados do que outros classificadores.\n",
    "        </li>\n",
    "        <li>\n",
    "            O algoritmo possui três <em>flavours </em>, o Gaussian, o Bernoulli e o Multinomial. Por ora, nos atentaremos apenas ao primeiro, enquanto os dois últimos serão abordados no curso de NLP.\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e7153718-7f1c-4e32-86e0-1c0e2e0d3fa5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.84"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import make_classification#, make_blobs, make_friedman1, make_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "# Criando um dataset fictício para classificação.\n",
    "X_C2, y_C2 = make_classification(n_samples = 100, n_features=2,\n",
    "                                n_redundant=0, n_informative=2,\n",
    "                                n_clusters_per_class=1, flip_y = 0.1,\n",
    "                                class_sep = 0.5, random_state=0)\n",
    "\n",
    "# Segregando os dados com 'train_test_split'\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_C2, y_C2, random_state=0)\n",
    "gaussian = GaussianNB().fit(X_train, y_train)\n",
    "gaussian.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a692b38-220e-4dbb-ad90-7ca054a6be07",
   "metadata": {},
   "source": [
    "<center>\n",
    "    <h1>Prós e Contras de Naïve Bayes</h1>\n",
    "    <img src='bayes1.png'>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e789681b-7fde-4d19-9576-885989b59abf",
   "metadata": {},
   "source": [
    "<div>\n",
    "    <hr>\n",
    "    <h2 style='font-size:30px'> Random Forests</h2>   \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32a0fa30-50e9-43b2-b273-69b6e04e3da4",
   "metadata": {},
   "source": [
    "<div>\n",
    "    <ul style='font-size:20px'>\n",
    "        <li>\n",
    "            O Random Forests consiste na realização de várias Decision Trees em um único dataset. Pode ser utilizado tanto para classificação, quanto regressão.\n",
    "        </li>\n",
    "        <li>\n",
    "            A quantidade de árvores da floresta será definida pelo parâmetro <em>n_estimator</em>. Por fim, os parâmetros de segregação de dados (os que criam os diferentes nodes da árvore) também serão definidos ao acaso; no entanto, podemos definir a quantidade de parâmetros a serem utilizados em cada árvore com o argumento <em>max_features</em>.\n",
    "        </li>\n",
    "        <li>\n",
    "            Vale ressaltar que cada árovre terá o seu próprio dataset de treino, contendo linhas aletórias e até mesmo repetidas do <em>X_train</em>.\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>\n",
    "<div>\n",
    "    <h3 style='font-size:30px;font-style:italic'> Resultado do algoritmo</h3>\n",
    "    <ul style='font-size:20px'> \n",
    "        <li>\n",
    "            Para regressão, o valor final da instância será a média de todas as previsões das árvores. Já com a classificação, se pegará as probabilidades de ocorrência de cada classe retornadas pelas árvores. Ao final, se calculará a média delas e a categoria com o maior probabilidade média será designada à instância.\n",
    "        </li>\n",
    "        <li>\n",
    "            É importante destacar que o objetivo do Random Forests é de criar um <em> ensemble</em> de modelos. Com isso, as forças de cada um serão combinadas e suas fraquezas minimizadas (como, por exemplo, a alta probabilidade de overfitting de uma única Decision Tree) . Isso é uma prática comum em ML.\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "35e51548-c997-4635-a85b-2b51a2081a22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test score when n_estimators=5: 80.00%\n",
      "Test score when n_estimators=10: 86.67%\n",
      "Test score when n_estimators=50: 80.00%\n",
      "Test score when n_estimators=100: 86.67%\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Utilizando o dataset de frutas.\n",
    "fruits = pd.read_table('fruit_data_with_colors (1).txt')\n",
    "X, y = fruits[['mass', 'width', 'height', 'color_score']], fruits['fruit_label']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 0)\n",
    "\n",
    "# Definindo vários números de árvores para as florestas.\n",
    "for n_estimators in [5, 10, 50, 100]:\n",
    "    clf = RandomForestClassifier(n_estimators=n_estimators, random_state=0).fit(X_train, y_train)\n",
    "    # Verificando a precisão de cada floresta.\n",
    "    print(f'Test score when n_estimators={n_estimators}: {clf.score(X_test, y_test):.2%}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e61bc69e-1a41-4683-978f-d010e9809450",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score em treino: 99.8%\n",
      "Score em teste: 95.1%\n"
     ]
    }
   ],
   "source": [
    "# Agora, teremos a aplicação do Random Forests no dataset de câncer de mama.\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "\n",
    "X, y = load_breast_cancer(return_X_y=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, random_state=0)\n",
    "\n",
    "# Criando o modelo de Random Forests com algumas regularizations.\n",
    "clf = RandomForestClassifier(n_estimators = 10, max_features=8).fit(X_train, y_train)\n",
    "print(f'Score em treino: {clf.score(X_train, y_train) :.1%}')\n",
    "print(f'Score em teste: {clf.score(X_test, y_test) :.1%}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb6f2d28-c70b-400f-a8b3-30f82896a801",
   "metadata": {},
   "source": [
    "<center>\n",
    "    <h1>Prós e Contras do Random Forest </h1>\n",
    "    <img src='random_forest1.png'>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf8f57d3-1dcb-4d21-bf01-31ae9d3ff615",
   "metadata": {},
   "source": [
    "<center>\n",
    "    <h1>Principais parâmetros do Random Forest </h1>\n",
    "    <img src='random_forest2.png'>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5227531f-4678-4c79-a759-0941ee486890",
   "metadata": {},
   "source": [
    "<div>\n",
    "    <hr>\n",
    "    <h2 style='font-size:30px'>Gradient Boosted Decision Trees </h2>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3896de6-da45-4c2e-98a9-562a5d603d18",
   "metadata": {},
   "source": [
    "<div>\n",
    "    <ul style='font-size:20px'>\n",
    "        <li>\n",
    "            O Gradient Boosted Decision Trees é um outro algoritmo que produz múltiplas Árvores de Decisão. No entanto, diferentemente do Random Forest, elas não são criadas de maneira aleaória. Cada nova árvore gerada tentará corrigir os erros da anterior.\n",
    "        </li>\n",
    "        <li>\n",
    "            O quão rigoroso será esse aperfeiçoamento é definido pelo parâmetro <em>learning rate</em>.\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bfc17469-cf15-4248-bd13-c223924d2518",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.76"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "X_D2, y_D2 = make_blobs(n_samples = 100, n_features = 2,\n",
    "                       centers = 8, cluster_std = 1.3,\n",
    "                       random_state = 4)\n",
    "y_D2 = y_D2 % 2\n",
    "\n",
    "# Segregando os dados.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_D2, y_D2, random_state=0)\n",
    "clf = GradientBoostingClassifier().fit(X_train, y_train)\n",
    "clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f1ed01e0-0cde-49eb-996f-28f1acc3ffe5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score de treino: 100.00%\n",
      "Score de teste: 96.50%\n"
     ]
    }
   ],
   "source": [
    "# Agora, aplicando o algoritmo no dataset de câncer de mama.\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "X, y = load_breast_cancer(return_X_y=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
    "\n",
    "clf = GradientBoostingClassifier(random_state=0).fit(X_train, y_train)\n",
    "print(f'Score de treino: {clf.score(X_train, y_train) :.2%}')\n",
    "print(f'Score de teste: {clf.score(X_test, y_test) :.2%}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6ba44b1f-94a4-4c10-869e-bd4191904711",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score de treino: 97.42%\n",
      "Score de teste: 96.50%\n"
     ]
    }
   ],
   "source": [
    "# Agora, vamos modificar alguns parâmetros do modelo.\n",
    "clf = GradientBoostingClassifier(learning_rate=0.01, max_depth=2, random_state=0).fit(X_train, y_train)\n",
    "print(f'Score de treino: {clf.score(X_train, y_train) :.2%}')\n",
    "print(f'Score de teste: {clf.score(X_test, y_test) :.2%}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9d467ec-8602-45b0-a103-1439d51f79c4",
   "metadata": {},
   "source": [
    "<center>\n",
    "    <h1> Prós e Contras do Gradient Boosting</h1>\n",
    "    <img src='gradient1.png'>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16bd2cee-b980-4858-a7d9-03050dd9017a",
   "metadata": {},
   "source": [
    "<center>\n",
    "    <h1> Principais Parâmetros do Gradient Boosting</h1>\n",
    "    <img src='gradient2.png'>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c3327fb-f555-4bb2-8613-b2972309136d",
   "metadata": {},
   "source": [
    "<div>\n",
    "    <hr>\n",
    "    <h2 style='font-size:30px'> Neural Networks</h2>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7738626-2eb3-4176-b3ce-975b2157407d",
   "metadata": {},
   "source": [
    "<h3 style='font-size:30px;font-style:italic'>Relembrando Conceitos</h3>\n",
    "<center>\n",
    "    <img src='neural1.png'>\n",
    "</center>\n",
    "<div>\n",
    "    <ul style='font-size:20px'>\n",
    "        <li>\n",
    "            Aprendemos que os algoritmos de Regressão criam uma fórmula em que cada <em>feature</em> presente no dataset tem um peso correspondente à sua relevância na previsão dos números. Ao olharmos para as regressões logísticas, notamos que elas dão um passo além, aplicando a fórmula da regressão linear à função <em>logistic </em>, o que vai fazer com que todos os seus outputs estejam no intervalo [0,1].\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "877dadcf-d326-4a9d-bdaf-a6f31c607b42",
   "metadata": {},
   "source": [
    "<h3 style='font-size:30px;font-style:italic'>As redes neurais</h3>\n",
    "<center>\n",
    "    <img src='neural2.png'>\n",
    "</center>\n",
    "<div>\n",
    "    <ul style='font-size:20px'> \n",
    "        <li>\n",
    "            As redes neurais, por sua vez, vão ainda mais além, em termos de complexidade, do que a regressão logística. Os valores de input são aplicados a uma camada do algoritmo conhecida como <em>hidden layer</em>; para cada camada, um coeficiente diferente é associado à variável, o que dará origem a uma fórmula de soma ponderada. Essa soma será aplicada a uma função conhecida como <em>activation function</em>. Por último, o output da <em>activation function</em> de cada camada será associado a um novo coeficiente, o que se resultará em uma nova fórmula de soma ponderada. Essa última fórmula é aquela com que o modelo fará as suas previsões.\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8314e2fe-3932-4fa2-926a-a2ea5a4a2ed7",
   "metadata": {},
   "source": [
    "<center>\n",
    "    <h1> Algumas das <em>Activation Functions</em> utilizáveis</h1>\n",
    "    <img src='neural3.png'>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dbd5ae25-0163-4c81-b44c-30e30f9cd197",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/felipeveiga/opt/anaconda3/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:500: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.72"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Aqui, trabalharemos com os Perceptrons.\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "X_D2, y_D2 = make_blobs(n_samples = 100, n_features = 2,\n",
    "                       centers = 8, cluster_std = 1.3,\n",
    "                       random_state = 4)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_D2, y_D2, random_state=0)\n",
    "\n",
    "# O argumento 'hidden_layer_sizes' definirá o número de 'hidden units' da 'hidden layer' (os bloquinhos\n",
    "# h1, h2, etc da penúltima imagem). Isso, por consequência, definirá a quantidade de argumentos que a função final\n",
    "# da rede neural receberá.\n",
    "\n",
    "# O 'solver' do modelo terá o papel de definir os coeficientes da função do modelo. Para datasets volumosos,\n",
    "# 'adam' é a escolha indicada; para os menores 'lbfgs' tende a ser mais rápido.\n",
    "\n",
    "# Os coeficientes do modelo são inicializados aleatoriamente, influenciados pela semente\n",
    "# do argumento 'random_state'.\n",
    "clf = MLPClassifier(hidden_layer_sizes=[10], solver='lbfgs', random_state=0).fit(X_train, y_train)\n",
    "clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5317ddb7-bd49-48e2-9f8e-311e13025e23",
   "metadata": {},
   "source": [
    "<center>\n",
    "    <img src='neural4.png'>\n",
    "</center>\n",
    "<ul style='font-size:20px'>\n",
    "    <li>\n",
    "        É possível criarmos mais de uma <em>hidden layer</em> para o algoritmo também.\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "61de0a69-d483-4711-b46c-ca0574006c43",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/felipeveiga/opt/anaconda3/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:500: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.72"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Criando um Multi-Layer Perceptron\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "X_D2, y_D2 = make_blobs(n_samples = 100, n_features = 2,\n",
    "                       centers = 8, cluster_std = 1.3,\n",
    "                       random_state = 4)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_D2, y_D2, random_state=0)\n",
    "\n",
    "# O procedimento é o mesmo da última célula, com o diferencial que 'hidden_layer_sizes' receberá mais \n",
    "# de uma integer.\n",
    "clf = MLPClassifier(hidden_layer_sizes=[10, 10], solver='lbfgs', random_state=0).fit(X_train, y_train)\n",
    "clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edecc4e7-5abc-4b80-9bbf-d401711c77d0",
   "metadata": {},
   "source": [
    "<h3 style='font-size:30px;font-style:italic'>As consequências de múltiplas hidden layers</h3>\n",
    "<div>\n",
    "    <ul style='font-size:20px'>\n",
    "        <li>\n",
    "            É preciso ser cauteloso com o número de camadas que nossa rede neural terá. Ao tornar o modelo mais complexo, o risco de <em>overfitting</em> aumenta, sendo conveniente a aplicação de regularizações.\n",
    "        </li>\n",
    "        <li>\n",
    "            A imagem abaixo apresenta as áreas de decisão para dois MLP, um com uma única camada, e o outro com uma segunda <em>hidden layer</em>.\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>\n",
    "<center>\n",
    "    <img src='neural5.png'>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "36faac5a-93ae-4044-a53e-bc8e24126cae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/felipeveiga/opt/anaconda3/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:500: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score no set de treino quando alpha=0.01: 0.9866666666666667\n",
      "Score no set de teste quando alpha=0.01: 0.76\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/felipeveiga/opt/anaconda3/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:500: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score no set de treino quando alpha=0.1: 0.9866666666666667\n",
      "Score no set de teste quando alpha=0.1: 0.72\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/felipeveiga/opt/anaconda3/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:500: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score no set de treino quando alpha=1.0: 0.9333333333333333\n",
      "Score no set de teste quando alpha=1.0: 0.72\n",
      "Score no set de treino quando alpha=5.0: 0.8666666666666667\n",
      "Score no set de teste quando alpha=5.0: 0.76\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/felipeveiga/opt/anaconda3/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:500: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    }
   ],
   "source": [
    "# O objeto MLPClassifier possui, assim como os métodos de regressão, um argumento 'alpha', que é capaz\n",
    "# de restringir a complexidade do algoritmo.\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import make_blobs\n",
    "X_D2, y_D2 = make_blobs(n_samples = 100, n_features = 2, centers = 8, \n",
    "                        cluster_std = 1.3, random_state = 4)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_D2, y_D2, random_state=0)\n",
    "\n",
    "# Criando Multi-Layer Perceptrions com diferentes valores de 'alpha'.\n",
    "for alpha in [0.01, 0.1, 1.0, 5.0]:\n",
    "    clf = MLPClassifier(hidden_layer_sizes=[100,100], activation='tanh', solver='lbfgs', \n",
    "                      alpha=alpha, random_state=0).fit(X_train, y_train)\n",
    "    print(f'Score no set de treino quando alpha={alpha}: {clf.score(X_train, y_train)}')\n",
    "    print(f'Score no set de teste quando alpha={alpha}: {clf.score(X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e789e5eb-c2e0-434d-af74-c940bee834c5",
   "metadata": {},
   "source": [
    "<center>\n",
    "    <h1>Impacto de <em>alpha</em> nas fronteiras de decisão</h1>\n",
    "    <img src='neural6.png'>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7e3374d2-0cb0-412e-a020-66f5ba9f4578",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score set de treino 0.9835680751173709\n",
      "Score set de teste 0.965034965034965\n"
     ]
    }
   ],
   "source": [
    "# Aplicação final de um MLPClassifier em um dataset real.\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "X, y = load_breast_cancer(return_X_y=True)\n",
    "\n",
    "# Aplicando um MinMaxScaler em X.\n",
    "scaler = MinMaxScaler()\n",
    "X_min_max = scaler.fit_transform(X)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_min_max, y, random_state=0)\n",
    "\n",
    "# Criando o MLPClassifier\n",
    "clf = MLPClassifier(hidden_layer_sizes=[100,100],activation='tanh', solver='lbfgs',\n",
    "           alpha=5, random_state=0).fit(X_train, y_train)\n",
    "print(f'Score set de treino {clf.score(X_train, y_train)}')\n",
    "print(f'Score set de teste {clf.score(X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2801484d-ea0a-48a0-9000-61fe91a4d2f1",
   "metadata": {},
   "source": [
    "<h3 style='font-size:30px;font-style:italic'>MLPRegressor</h3>\n",
    "<div>\n",
    "    <ul style='font-size:20px'>\n",
    "        <li>\n",
    "            Os Multi Layer Perceptrons também são aplicáveis a problemas de Regressão. Faremos brevemente um exemplo de uso do MLPRegressor.\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "032e3b38-60d3-4b2b-a9c0-21e0d697aa81",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/felipeveiga/opt/anaconda3/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:500: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error quando alpha=0.01 e activation=tanh: 1246.362648845935\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/felipeveiga/opt/anaconda3/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:500: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error quando alpha=0.1 e activation=tanh: 1423.4965262385622\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/felipeveiga/opt/anaconda3/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:500: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error quando alpha=1.0 e activation=tanh: 1327.34559786546\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/felipeveiga/opt/anaconda3/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:500: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error quando alpha=5.0 e activation=tanh: 1278.1597756154995\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/felipeveiga/opt/anaconda3/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:500: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error quando alpha=0.01 e activation=relu: 1275.7502426033227\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/felipeveiga/opt/anaconda3/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:500: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error quando alpha=0.1 e activation=relu: 1236.6394415677203\n",
      "Mean Squared Error quando alpha=1.0 e activation=relu: 1205.8426210543153\n",
      "Mean Squared Error quando alpha=5.0 e activation=relu: 1238.170940050707\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/felipeveiga/opt/anaconda3/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:500: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import make_regression \n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "\n",
    "X_R1, y_R1 = make_regression(n_samples = 100, n_features=1, n_informative=1, bias = 150.0,\n",
    "                            noise = 30, random_state=0)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_R1, y_R1, random_state=0)\n",
    "\n",
    "for activation in ['tanh', 'relu']:\n",
    "    for alpha in [0.01, 0.1, 1.0, 5.0]:\n",
    "        reg = MLPRegressor(hidden_layer_sizes=[100,100], activation=activation, solver='lbfgs',\n",
    "                          alpha=alpha, random_state=0).fit(X_train, y_train)\n",
    "        y_predict = reg.predict(X_test)\n",
    "        print(f'Mean Squared Error quando alpha={alpha} e activation={activation}: {mean_squared_error(y_test, y_predict)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dacf1f01-ee48-4629-af9e-49483025e44a",
   "metadata": {},
   "source": [
    "<center>\n",
    "    <h1> Prós e contras das redes neurais</h1>\n",
    "    <img src='neural7.png'>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a331d06-b6dc-45ce-b512-feaaf07ce2ca",
   "metadata": {},
   "source": [
    "<div>\n",
    "    <hr>\n",
    "    <h2 style='font-size:30px'> Deep Learning</h2>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1661545-4ba8-4bba-951a-d0a7a0b18bed",
   "metadata": {},
   "source": [
    "<div>\n",
    "    <ul style='font-size:20px'>\n",
    "        <li>\n",
    "            O Deep Learning é uma área do aprendizado de máquina cuja origem provém de uma das maiores dificuldades do campo: a engenharia de dados. Ou seja, o processo que almeja a seleção e recorte dos dados que melhor alimentem o modelo escolhido\n",
    "        </li>\n",
    "        <li>\n",
    "            Os algoritmos de Deep Learning são capazes de, por conta própria, segmentar o dataset e buscar as combinações que melhor preparem o algoritmo.\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71e5de33-69fe-426b-8e16-863034826c5a",
   "metadata": {},
   "source": [
    "<center> \n",
    "    <img src='deep1.png'>\n",
    "</center>\n",
    "<div>\n",
    "    <ul style='font-size:20px'> \n",
    "        <li>\n",
    "            No reconhecimento de faces, as fotografias do dataset de treino são quebradas em pedaços minúsculos. Dessa forma, cada camada do modelo fará pequenas combinações entre esses fragmentos ate que sejam capazes de formar imagens verossímeis de rostos.\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f0eeca4-38f5-4702-9395-fdfaf073d749",
   "metadata": {},
   "source": [
    "<center> \n",
    "    <h1> Prós e Contras do Deep Learning</h1>\n",
    "    <img src='deep2.png'>\n",
    "</center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e09cc5e5-2395-455e-b454-d5cd171f5683",
   "metadata": {},
   "source": [
    "<div>\n",
    "    <hr>\n",
    "    <h2 style='font-size:30px'> Data Leakage</h2>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c344b7f-c7f7-49d9-9fbe-ed6a5ae18ebb",
   "metadata": {},
   "source": [
    "<div>\n",
    "    <ul style='font-size:20px'> \n",
    "        <li>\n",
    "            O Data Leakage é a situação em que dados sobre o que estamos tentando prever surgem durante a fase de treino do modelo. Ademais, deve-se considerar que essas mesmas informações não estarão dsponíveis ao nosso modelo no momento em que ele for colocado em ação.\n",
    "        </li>\n",
    "        <li>\n",
    "            Deve-se suspeitar a ocorrênca de um Leakage quando o algoritmo tem uma performance muito pior no seu deployment do que quando na fase de treino e teste.\n",
    "        </li>\n",
    "        <li> \n",
    "            Casos de Data Leakage:\n",
    "            <ul style='list-style-type:lower-alpha'> \n",
    "                <li>\n",
    "                    O rótulo ou valor das instâncias do dataset de treino (y_train) surgem como features no X_train.\n",
    "                </li>\n",
    "                <li>\n",
    "                    Instâncias do dataset de teste estão contidas no dataset de treino.\n",
    "                </li>\n",
    "                <li>\n",
    "                    Features do dataset de treino/teste que não estarão ao dispor do modelo quando posto em ação. Por exemplo, se queremos ter um sistema que prevê se um usuário abandonará o site que está visitando enquanto o navega, ter um dataset com o tempo total de navegação de usuários anteriores não é algo legítmo. Afinal de contas, essa feature não existirá enquanto o indivíduo não sair do site.\n",
    "                </li>\n",
    "                <li>\n",
    "                    Features que, indiretamente, já indicam o valor ou categoria do dado que queremos prever. Neste caso, é necessário ter bastante atenção e raciocínio quando se observa o dataset. \n",
    "                </li>\n",
    "                <li>\n",
    "                    Informações decodificadas. Às vezes, decodificar dados no dataset pode ser inútil ou prejudicial, pois essas informações podem não estar presentes ao modelo quando em ação justamente por virem criptografadas ou escondidas. \n",
    "                </li>\n",
    "            </ul>\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcb3eec3-17c4-4f58-9192-5032f44b1bbf",
   "metadata": {},
   "source": [
    "<h3 style='font-size:30px;font-style:italic'>Como identificar data leakages</h3>\n",
    "<div>\n",
    "    <ul style='font-size:20px'>\n",
    "        <li>\n",
    "            Verificar features muito correlatas com o valor-alvo.\n",
    "        </li>\n",
    "        <li>\n",
    "            Features com pesos muito altos podem ser um indicativo de Leakage.\n",
    "        </li>\n",
    "        <li>\n",
    "            Checar se a performance do nosso modelo é muito superior à de outros algoritmos no mesmo dataset.\n",
    "        </li>\n",
    "        <li>\n",
    "            Colocar o modelo em vigor. Caso o desempenho seja muito pior do que durante a fase de treino e teste, um Leakage é altamente provável de ter acontecido.\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58b9bf07-f46c-453e-8094-93de6d5908e9",
   "metadata": {},
   "source": [
    "REVER A PALESTRA DE DATA LEAKAGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4b2a0b9a-0d05-482c-90e0-093aeb387dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "! mv /Users/felipeveiga/Desktop/Screen\\ Shot\\ 2022-04-12\\ at\\ 13.02.56.png ./deep2.png"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
