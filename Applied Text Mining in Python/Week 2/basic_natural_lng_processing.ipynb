{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5c1fbef3-8ea0-44a6-9f70-5c161648f715",
   "metadata": {
    "tags": []
   },
   "source": [
    "<h1 style='font-size:40px'> Basic Natural Language Processing</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ee6eb6e-fdc3-4f65-aeab-231e253e1028",
   "metadata": {},
   "source": [
    "<h2 style='font-size:30px'> Basic NLP tasks with NLTK</h2>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li> \n",
    "            A biblioteca do NLTK possui um conjunto de textos-exemplo disponíveis para que o usuário treine as suas habilidades com NLP.\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "544d99a4-0c57-40b0-a785-a7a507e13dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importando os textos de exemplo da biblioteca.\n",
    "import nltk\n",
    "from nltk.book import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "73fba2c5-1a4c-4666-a860-0f684e6fb300",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sent1: Call me Ishmael .\n",
      "sent2: The family of Dashwood had long been settled in Sussex .\n",
      "sent3: In the beginning God created the heaven and the earth .\n",
      "sent4: Fellow - Citizens of the Senate and of the House of Representatives :\n",
      "sent5: I have a problem with people PMing me to lol JOIN\n",
      "sent6: SCENE 1 : [ wind ] [ clop clop clop ] KING ARTHUR : Whoa there !\n",
      "sent7: Pierre Vinken , 61 years old , will join the board as a nonexecutive director Nov. 29 .\n",
      "sent8: 25 SEXY MALE , seeks attrac older single lady , for discreet encounters .\n",
      "sent9: THE suburb of Saffron Park lay on the sunset side of London , as red and ragged as a cloud of sunset .\n",
      "None\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Pierre',\n",
       " 'Vinken',\n",
       " ',',\n",
       " '61',\n",
       " 'years',\n",
       " 'old',\n",
       " ',',\n",
       " 'will',\n",
       " 'join',\n",
       " 'the',\n",
       " 'board',\n",
       " 'as',\n",
       " 'a',\n",
       " 'nonexecutive',\n",
       " 'director',\n",
       " 'Nov.',\n",
       " '29',\n",
       " '.']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 'sents' é um método que nos revela uma frase pertencente a cada texto da coleção.\n",
    "print(sents())\n",
    "\n",
    "# Podemos acessar uma das sentenças disponíveis com as variáveis 'sent{numero_do_texto}'.\n",
    "sent7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6ba7a7a-653a-41c1-8edf-dea2e82dac7e",
   "metadata": {},
   "source": [
    "<h3 style='font-size:30px;font-style:italic'>FreqDist</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7d5238ef-053b-459a-93e1-b8756f84bc7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Para essa parte introdutória à biblioteca, nos atentaremos ao text7, cuja fonte é o The Wall Street Journal.\n",
    "\n",
    "# A classe 'FreqDist' retorna uma espécie de dicionário com a contagem de ocorrência de cada token no texto.\n",
    "dist = FreqDist(text7)\n",
    "\n",
    "# Quantas vezes a palavra 'dollar' aparece no excerto?\n",
    "dist['dollar']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "195b629b-35e9-4577-862e-19bfad04bfae",
   "metadata": {},
   "source": [
    "<h3 style='font-size:30px;font-style:italic'>Normalization and Stemming</h3>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li> \n",
    "            O processo de normalização dos termos de um texto é algo extremamente recomendável em tarefas de NLP. Por exemplo, a palavra 'List' e 'list', apesar de significarem a mesma coisa, não são consideradas iguais pelo computador por estarem escritas de maneira distinta.\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2f0114d3-cd74-46c5-ad6b-8d039031a219",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['list', 'listed', 'listing', 'list']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Se quiséssemos analisar as palavas da lista 'roots', por exemplo, seria importante colocarmos todas as palavras em letra minúscula.\n",
    "roots = ['list', 'listed', 'listing', 'List']\n",
    "roots = [word.lower() for word in roots]\n",
    "roots"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "794734e3-4123-48d3-9073-75b2e6c87fea",
   "metadata": {},
   "source": [
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li> \n",
    "            'Stem', do inglês, quer dizer 'tronco'. Gramaticamente, procurar o 'stem' de uma palavra é equivalente a descobrir o que chamamos de <strong>raiz</strong>.\n",
    "        </li>\n",
    "        <li> \n",
    "            Em análise de textos, às vezes é interessante que convertamos palavras distintas com mesma raiz em uma única só.\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "00032aef-4a53-429a-93f1-0799e6e9f5fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['list', 'list', 'list', 'list']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'univers'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Qual é a raiz dos termos de 'roots'?\n",
    "porter = nltk.PorterStemmer()\n",
    "\n",
    "# No caso do algoritmo Porter Stemmer, todas as variações e 'list' são transformadas. É uma abordagem radical que nem sempre desejamos.\n",
    "# Além disso, esse pode nos retornar palavras inexistentes na língua inglesa, dependendo do termo que sofre a normalização. Veja o que\n",
    "# ocorre com 'universal'.\n",
    "print([porter.stem(word) for word in roots])\n",
    "porter.stem('universal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0680704f-c587-493c-a6af-61f761fa7004",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['dollar', 'dollar']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Outro exemplo é quando lidamos com palavras no plural.\n",
    "# Não seria mais sensato considerarmos 'dollar' e 'dollars' como a mesma coisa?\n",
    "money = ['dollar', 'dollars']\n",
    "[porter.stem(word) for word in money]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f356b7c-a2dc-4c3e-9a94-8909bc8a93d6",
   "metadata": {},
   "source": [
    "<h3 style='font-size:30px;font-style:italic'>Lemmatization</h3>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li> \n",
    "            É considerando esse retorno possivelmente indesejado de Porter que existe o processo de Lemmatização. O seu algoritmo no nltk retorna uma palavra existente quando fazemos um stemming.\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9310d61d-2f28-438b-9ba9-a90b2d72ce4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Universal',\n",
       " 'Declaration',\n",
       " 'of',\n",
       " 'Human',\n",
       " 'Rights',\n",
       " 'Preamble',\n",
       " 'Whereas',\n",
       " 'recognition',\n",
       " 'of',\n",
       " 'the',\n",
       " 'inherent',\n",
       " 'dignity',\n",
       " 'and',\n",
       " 'of',\n",
       " 'the',\n",
       " 'equal',\n",
       " 'and',\n",
       " 'inalienable',\n",
       " 'right',\n",
       " 'of']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "udhr = nltk.corpus.udhr.words('English-Latin1')\n",
    "WNLemma = nltk.WordNetLemmatizer()\n",
    "\n",
    "# Fazendo a 'Lemmatização' das primeiras 20 palavras da DUDH.\n",
    "[WNLemma.lemmatize(word) for word in udhr[:20]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59531f24-0b61-45e2-9a37-3c38a36e37bf",
   "metadata": {},
   "source": [
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li> \n",
    "            Mas observe que, por exemplo, a palavra \"Rights\" não sofreu o stemming desejado. Isso porque ela não está em minúsculas! Ou seja, para que a lemmatização seja feita, é preciso normalizar as palavras,\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "63c46170-d074-471a-a423-6dedcb6e8829",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['universal',\n",
       " 'declaration',\n",
       " 'of',\n",
       " 'human',\n",
       " 'right',\n",
       " 'preamble',\n",
       " 'whereas',\n",
       " 'recognition',\n",
       " 'of',\n",
       " 'the',\n",
       " 'inherent',\n",
       " 'dignity',\n",
       " 'and',\n",
       " 'of',\n",
       " 'the',\n",
       " 'equal',\n",
       " 'and',\n",
       " 'inalienable',\n",
       " 'right',\n",
       " 'of']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Normalizando os termos.\n",
    "udhr_lower = [word.lower() for word in udhr]\n",
    "[WNLemma.lemmatize(word) for word in udhr_lower[:20]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42179c2a-2229-4d31-a9dc-a1a9785b7301",
   "metadata": {},
   "source": [
    "<h3 style='font-size:30px;font-style:italic'>Tokenization</h3>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li> \n",
    "            Como verificamos em outras oportunidades, separar termos de frases com um \"split\" pode nos gerar resultados incorretos. Palavras podem ter sinais de pontuação inclusos, por exemplo.\n",
    "        </li>\n",
    "        <li> \n",
    "            É considerando essas limitações que a NLTK tem um método próprio para tokenização.\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "59065c66-fdac-413a-8b08-718fb06903c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Children', \"shouldn't\", 'drink', 'a', 'sugary', 'drink', 'before', 'bed.']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dividindo os termos de 'text11' de acordo com um ' '. O resultado é imperfeito.\n",
    "text11 = \"Children shouldn't drink a sugary drink before bed.\"\n",
    "text11.split(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aaab226d-ae2c-4644-b22d-82bbdd4fbd8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Children',\n",
       " 'should',\n",
       " \"n't\",\n",
       " 'drink',\n",
       " 'a',\n",
       " 'sugary',\n",
       " 'drink',\n",
       " 'before',\n",
       " 'bed',\n",
       " '.']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# O NLTK não foi apenas capaz de separar 'bed' do '.', como também separou a negação 'n't' de 'should'.\n",
    "nltk.word_tokenize(text11)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db99f54c-8660-46ef-b7a3-12fa3c38bca2",
   "metadata": {},
   "source": [
    "<h3 style='font-size:30px;font-style:italic'>Sentence Splitting</h3>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li> \n",
    "            Dividir os períodos de uma frase pode ser algo bastante complexo para um computador. Isso porque um '.' não indica necessariamente o final de um período.\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "01bf403f-705e-4b82-a5bd-1a8e32ff674c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This is the first sentence',\n",
       " ' A gallon of milk in the U',\n",
       " 'S',\n",
       " ' costs $2',\n",
       " '99',\n",
       " ' Is this the third sentence? Yes, it is!']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tendo a seguinte frase em mãos, vamos dividí-la com um 'split('.')'.\n",
    "text12 = \"This is the first sentence. A gallon of milk in the U.S. costs $2.99. Is this the third sentence? Yes, it is!\"\n",
    "\n",
    "# O resultado é uma separação bizarra de 'text12'.\n",
    "text12.split('.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0875733a-7e6c-4097-a8cd-a0bf95da1dab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This is the first sentence.',\n",
       " 'A gallon of milk in the U.S. costs $2.99.',\n",
       " 'Is this the third sentence?',\n",
       " 'Yes, it is!']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# É considerando essa dificuldade que a nltk possui, também, um separador de frases.\n",
    "nltk.sent_tokenize(text12)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00e94d74-457d-4b39-b38f-03de19fc783e",
   "metadata": {},
   "source": [
    "<div> \n",
    "    <hr>\n",
    "    <h2 style='font-size:30px'>Advanced NLP tasks with NLTK </h2>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "713725b3-9db5-4bc0-a25e-2a4145f05993",
   "metadata": {},
   "source": [
    "<h3 style='font-size:30px;font-style:italic'> Identificação de classes gramaticais</h3>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li> \n",
    "            A Universidade da Pensilvânia criou códigos de identificação para cada classe gramatical da língua inglesa.\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>\n",
    "<center> \n",
    "    <img src='class_gramar1.png'>\n",
    "</center>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li> \n",
    "            Ao fazermos uma análise de classes gramaticais com o NLTK, essas tags serão o output do método correspondente.\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "165c787e-aaa1-4a92-bd36-7a13bcf6c8eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Children',\n",
       " 'should',\n",
       " \"n't\",\n",
       " 'drink',\n",
       " 'a',\n",
       " 'sugary',\n",
       " 'drink',\n",
       " 'before',\n",
       " 'bed',\n",
       " '.']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Para fazermos essa espécie de análise, precisamos primeiro tokenizar a frase.\n",
    "text11 = \"Children shouldn't drink a sugary drink before bed.\"\n",
    "text11_tokens = nltk.word_tokenize(text11)\n",
    "text11_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0f57dd23-b5fa-4569-9271-5ce999cb3fbf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Children', 'NNP'),\n",
       " ('should', 'MD'),\n",
       " (\"n't\", 'RB'),\n",
       " ('drink', 'VB'),\n",
       " ('a', 'DT'),\n",
       " ('sugary', 'JJ'),\n",
       " ('drink', 'NN'),\n",
       " ('before', 'IN'),\n",
       " ('bed', 'NN'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Obtendo as classes gramaticais de cada palavra.\n",
    "nltk.pos_tag(text11_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "64f8b0d6-de04-4ff6-9e2c-16673f6360ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JJ: adjective or numeral, ordinal\n",
      "    third ill-mannered pre-war regrettable oiled calamitous first separable\n",
      "    ectoplasmic battery-powered participatory fourth still-to-be-named\n",
      "    multilingual multi-disciplinary ...\n"
     ]
    }
   ],
   "source": [
    "# Se tiver dúvidas quanto ao significado de uma certa tag, use 'help.upenn_tagset'.\n",
    "nltk.help.upenn_tagset('JJ')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "062ad7fa-c61f-4764-b408-af470db09ffa",
   "metadata": {},
   "source": [
    "<h4 style='font-size:30px;font-style:italic;text-decoration:underline'> Ambiguidades com a classificação gramatical</h4>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li> \n",
    "            Como característica natural das línguas, palavras podem ter significados distintos a depender do contexto. Isso pode ser um fator que dificulta a correta identificação das classes gramaticais.\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>\n",
    "<center> \n",
    "    <img src='ambiguity.png'>\n",
    "</center>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li> \n",
    "            Observe a frase acima. \"Visiting\", sem um contexto definido, pode assumir papel tanto de um adjetivo, quanto de um verbo. À qual classe gramatical o nltk designará? Neste caso, como palavras com terminação em \"ing\" tendem a ser verbos no gerúndio, a biblioteca  definirá palavra como tal.\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a9f64ada-fb8c-4061-86d4-d977b28d6ece",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('You', 'PRP'),\n",
       " (\"'re\", 'VBP'),\n",
       " ('my', 'PRP$'),\n",
       " ('running', 'JJ'),\n",
       " ('dog', 'NN'),\n",
       " (',', ','),\n",
       " ('CJ', 'NNP'),\n",
       " ('!', '.')]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Mas veja, com o contexto esclarecido, o 'pos_tag' consegue identificar a classe correta.\n",
    "big_smoke = 'You\\'re my running dog, CJ!'\n",
    "tokenize_big_smoke = nltk.word_tokenize(big_smoke)\n",
    "nltk.pos_tag(tokenize_big_smoke)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "506300e7-a759-43f2-b421-e990fc0a7070",
   "metadata": {},
   "source": [
    "<h3 style='font-size:30px;font-style:italic'> Parsing Sentence Structure</h3>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li> \n",
    "            Analisar a estrutura de uma frase também é algo com um certo grau de complexidade, a depender do texto. Por isso, ao fazermos tal tarefa, é necessário criar ou possuir uma gramática com a qual o programa classificará os termos da sentença.\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "41df9b4a-df6f-4202-a142-5d6d443e15ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S (NP Alice) (VP (V loves) (NP Bob)))\n"
     ]
    }
   ],
   "source": [
    "# Analisando uma frase simples.\n",
    "text15 = 'Alice loves Bob'\n",
    "\n",
    "# Criando a gramática da frase.\n",
    "grammar = nltk.CFG.fromstring(''' \n",
    "                        S -> NP VP\n",
    "                        VP -> V NP\n",
    "                        NP -> 'Alice' | 'Bob'\n",
    "                        V -> 'loves' ''')\n",
    "\n",
    "# Gerando um analisador de frases.\n",
    "parser = nltk.ChartParser(grammar)\n",
    "trees = parser.parse_all(nltk.word_tokenize(text15))\n",
    "for tree in trees:\n",
    "    print(tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a513fed-5b26-453e-bebd-fc775da44a21",
   "metadata": {},
   "source": [
    "<h4 style='font-size:30px;font-style:italic;text-decoration:underline'> NLTK and Parse Tree Collection</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5ca03b71-495f-40a5-ad6e-7ed77ace18ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Tree('S', [Tree('NP-SBJ', [Tree('NP', [Tree('NNP', ['Pierre']), Tree('NNP', ['Vinken'])]), Tree(',', [',']), Tree('ADJP', [Tree('NP', [Tree('CD', ['61']), Tree('NNS', ['years'])]), Tree('JJ', ['old'])]), Tree(',', [','])]), Tree('VP', [Tree('MD', ['will']), Tree('VP', [Tree('VB', ['join']), Tree('NP', [Tree('DT', ['the']), Tree('NN', ['board'])]), Tree('PP-CLR', [Tree('IN', ['as']), Tree('NP', [Tree('DT', ['a']), Tree('JJ', ['nonexecutive']), Tree('NN', ['director'])])]), Tree('NP-TMP', [Tree('NNP', ['Nov.']), Tree('CD', ['29'])])])]), Tree('.', ['.'])]), Tree('S', [Tree('NP-SBJ', [Tree('NNP', ['Mr.']), Tree('NNP', ['Vinken'])]), Tree('VP', [Tree('VBZ', ['is']), Tree('NP-PRD', [Tree('NP', [Tree('NN', ['chairman'])]), Tree('PP', [Tree('IN', ['of']), Tree('NP', [Tree('NP', [Tree('NNP', ['Elsevier']), Tree('NNP', ['N.V.'])]), Tree(',', [',']), Tree('NP', [Tree('DT', ['the']), Tree('NNP', ['Dutch']), Tree('VBG', ['publishing']), Tree('NN', ['group'])])])])])]), Tree('.', ['.'])])]\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import treebank\n",
    "text17 = treebank.parsed_sents('wsj_0001.mrg')\n",
    "print(text17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "1095de7b-b9cd-4763-a169-88d54bd0e72b",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "findall() missing 2 required positional arguments: 'pattern' and 'string'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_267861/4212106084.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfindall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: findall() missing 2 required positional arguments: 'pattern' and 'string'"
     ]
    }
   ],
   "source": [
    "import re\n",
    "re.findall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e9d4f693-e366-41ea-a0a5-f433b3063be0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package tagsets to /home/veiga/nltk_data...\n",
      "[nltk_data]   Unzipping help/tagsets.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('tagsets')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5dc50d4-2eae-4089-a01d-63f801dc4b02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Questão 9 Assignment.\n",
    "def count_similarities(trigram1, trigram2):\n",
    "    count = 0\n",
    "    for letter1, letter2 in zip(trigram1, trigram2):\n",
    "        if letter1 == letter2:\n",
    "            count+=1\n",
    "    return count\n",
    "\n",
    "def answer_nine(entries=['cormulent', 'incendenece', 'validrate']):\n",
    "    l = []\n",
    "    # Extracting the trigrams from each word in entries.\n",
    "    for word in entries:\n",
    "        if word.endswith('ece'):\n",
    "            word = word[:-4]+'nce'\n",
    "            \n",
    "        trigram_first = word[:3]\n",
    "        trigram_last = word[-3:]\n",
    "        chosen_expression = None\n",
    "        chosen_jaccard = 1\n",
    "        # Examining the words in 'correct_spellings'\n",
    "        for expression in correct_spellings:\n",
    "            exp_trigram_first = expression[:3]\n",
    "            exp_trigram_last = expression[-3:]\n",
    "            \n",
    "            #Esta condição está radical demais. Vamos criar um método para contar o número\n",
    "            # de letras em comum por posição nos trigramas.\n",
    "            if count_similarities(exp_trigram_first, trigram_first)>=3 and count_similarities(exp_trigram_last, trigram_last)==3:\n",
    "                jaccard = nltk.jaccard_distance(set(word), set(expression))\n",
    "                if jaccard < chosen_jaccard:\n",
    "                    chosen_expression = expression\n",
    "                    chosen_jaccard = jaccard\n",
    "                \n",
    "        l.append(chosen_expression)\n",
    "    return list(l) # Your answer here\n",
    "    \n",
    "answer_nine()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59fc7f6a-f71e-4542-b3b4-9cb9128ec7f1",
   "metadata": {},
   "source": [
    "<a href='https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html'> UPenn</a>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
