{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5c1fbef3-8ea0-44a6-9f70-5c161648f715",
   "metadata": {},
   "source": [
    "<h1 style='font-size:40px'> Basic Natural Language Processing</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ee6eb6e-fdc3-4f65-aeab-231e253e1028",
   "metadata": {},
   "source": [
    "<h2 style='font-size:30px'> Basic NLP tasks with NLTK</h2>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li> \n",
    "            A biblioteca do NLTK possui um conjunto de textos-exemplo disponíveis para que o usuário treine as suas habilidades com NLP.\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "544d99a4-0c57-40b0-a785-a7a507e13dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importando os textos de exemplo da biblioteca.\n",
    "import nltk\n",
    "from nltk.book import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "73fba2c5-1a4c-4666-a860-0f684e6fb300",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sent1: Call me Ishmael .\n",
      "sent2: The family of Dashwood had long been settled in Sussex .\n",
      "sent3: In the beginning God created the heaven and the earth .\n",
      "sent4: Fellow - Citizens of the Senate and of the House of Representatives :\n",
      "sent5: I have a problem with people PMing me to lol JOIN\n",
      "sent6: SCENE 1 : [ wind ] [ clop clop clop ] KING ARTHUR : Whoa there !\n",
      "sent7: Pierre Vinken , 61 years old , will join the board as a nonexecutive director Nov. 29 .\n",
      "sent8: 25 SEXY MALE , seeks attrac older single lady , for discreet encounters .\n",
      "sent9: THE suburb of Saffron Park lay on the sunset side of London , as red and ragged as a cloud of sunset .\n",
      "None\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Pierre',\n",
       " 'Vinken',\n",
       " ',',\n",
       " '61',\n",
       " 'years',\n",
       " 'old',\n",
       " ',',\n",
       " 'will',\n",
       " 'join',\n",
       " 'the',\n",
       " 'board',\n",
       " 'as',\n",
       " 'a',\n",
       " 'nonexecutive',\n",
       " 'director',\n",
       " 'Nov.',\n",
       " '29',\n",
       " '.']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 'sents' é um método que nos revela uma frase pertencente a cada texto da coleção.\n",
    "print(sents())\n",
    "\n",
    "# Podemos acessar uma das sentenças disponíveis com as variáveis 'sent{numero_do_texto}'.\n",
    "sent7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6ba7a7a-653a-41c1-8edf-dea2e82dac7e",
   "metadata": {},
   "source": [
    "<h3 style='font-size:30px;font-style:italic'>FreqDist</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7d5238ef-053b-459a-93e1-b8756f84bc7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Para essa parte introdutória à biblioteca, nos atentaremos ao text7, cuja fonte é o The Wall Street Journal.\n",
    "\n",
    "# A classe 'FreqDist' retorna uma espécie de dicionário com a contagem de ocorrência de cada token no texto.\n",
    "dist = FreqDist(text7)\n",
    "\n",
    "# Quantas vezes a palavra 'dollar' aparece no excerto?\n",
    "dist['dollar']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "195b629b-35e9-4577-862e-19bfad04bfae",
   "metadata": {},
   "source": [
    "<h3 style='font-size:30px;font-style:italic'>Normalization and Stemming</h3>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li> \n",
    "            O processo de normalização dos termos de um texto é algo extremamente recomendável em tarefas de NLP. Por exemplo, a palavra 'List' e 'list', apesar de significarem a mesma coisa, não são consideradas iguais pelo computador por estarem escritas de maneira distinta.\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2f0114d3-cd74-46c5-ad6b-8d039031a219",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['list', 'listed', 'listing', 'list']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Se quiséssemos analisar as palavas da lista 'roots', por exemplo, seria importante colocarmos todas as palavras em letra minúscula.\n",
    "roots = ['list', 'listed', 'listing', 'List']\n",
    "roots = [word.lower() for word in roots]\n",
    "roots"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "794734e3-4123-48d3-9073-75b2e6c87fea",
   "metadata": {},
   "source": [
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li> \n",
    "            'Stem', do inglês, quer dizer 'tronco'. Gramaticamente, procurar o 'stem' de uma palavra é equivalente a descobrir o que chamamos de <strong>raiz</strong>.\n",
    "        </li>\n",
    "        <li> \n",
    "            Em análise de textos, às vezes é interessante que convertamos palavras distintas com mesma raiz em uma única só.\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "00032aef-4a53-429a-93f1-0799e6e9f5fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['list', 'list', 'list', 'list']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'univers'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Qual é a raiz dos termos de 'roots'?\n",
    "porter = nltk.PorterStemmer()\n",
    "\n",
    "# No caso do algoritmo Porter Stemmer, todas as variações e 'list' são transformadas. É uma abordagem radical que nem sempre desejamos.\n",
    "# Além disso, esse pode nos retornar palavras inexistentes na língua inglesa, dependendo do termo que sofre a normalização. Veja o que\n",
    "# ocorre com 'universal'.\n",
    "print([porter.stem(word) for word in roots])\n",
    "porter.stem('universal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "0680704f-c587-493c-a6af-61f761fa7004",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['dollar', 'dollar']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Outro exemplo é quando lidamos com palavras no plural.\n",
    "# Não seria mais sensato considerarmos 'dollar' e 'dollars' como a mesma coisa?\n",
    "money = ['dollar', 'dollars']\n",
    "[porter.stem(word) for word in money]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f356b7c-a2dc-4c3e-9a94-8909bc8a93d6",
   "metadata": {},
   "source": [
    "<h3 style='font-size:30px;font-style:italic'>Lemmatization</h3>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li> \n",
    "            É considerando esse retorno possivelmente indesejado de Porter que existe o processo de Lemmatização. O seu algoritmo no nltk retorna uma palavra existente quando fazemos um stemming.\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "9310d61d-2f28-438b-9ba9-a90b2d72ce4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Universal',\n",
       " 'Declaration',\n",
       " 'of',\n",
       " 'Human',\n",
       " 'Rights',\n",
       " 'Preamble',\n",
       " 'Whereas',\n",
       " 'recognition',\n",
       " 'of',\n",
       " 'the',\n",
       " 'inherent',\n",
       " 'dignity',\n",
       " 'and',\n",
       " 'of',\n",
       " 'the',\n",
       " 'equal',\n",
       " 'and',\n",
       " 'inalienable',\n",
       " 'right',\n",
       " 'of']"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "udhr = nltk.corpus.udhr.words('English-Latin1')\n",
    "WNLemma = nltk.WordNetLemmatizer()\n",
    "\n",
    "# Fazendo a 'Lemmatização' das primeiras 20 palavras da DUDH.\n",
    "[WNLemma.lemmatize(word) for word in udhr[:20]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59531f24-0b61-45e2-9a37-3c38a36e37bf",
   "metadata": {},
   "source": [
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li> \n",
    "            Mas observe que, por exemplo, a palavra \"Rights\" não sofreu o stemming desejado. Isso porque ela não está em minúsculas! Ou seja, para que a lemmatização seja feita, é preciso normalizar as palavras,\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "63c46170-d074-471a-a423-6dedcb6e8829",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['universal',\n",
       " 'declaration',\n",
       " 'of',\n",
       " 'human',\n",
       " 'right',\n",
       " 'preamble',\n",
       " 'whereas',\n",
       " 'recognition',\n",
       " 'of',\n",
       " 'the',\n",
       " 'inherent',\n",
       " 'dignity',\n",
       " 'and',\n",
       " 'of',\n",
       " 'the',\n",
       " 'equal',\n",
       " 'and',\n",
       " 'inalienable',\n",
       " 'right',\n",
       " 'of']"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Normalizando os termos.\n",
    "udhr_lower = [word.lower() for word in udhr]\n",
    "[WNLemma.lemmatize(word) for word in udhr_lower[:20]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42179c2a-2229-4d31-a9dc-a1a9785b7301",
   "metadata": {},
   "source": [
    "<h3 style='font-size:30px;font-style:italic'>Tokenization</h3>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li> \n",
    "            Como verificamos em outras oportunidades, separar termos de frases com um \"split\" pode nos gerar resultados incorretos. Palavras podem ter sinais de pontuação inclusos, por exemplo.\n",
    "        </li>\n",
    "        <li> \n",
    "            É considerando essas limitações que a NLTK tem um método próprio para tokenização.\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "59065c66-fdac-413a-8b08-718fb06903c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Children', \"shouldn't\", 'drink', 'a', 'sugary', 'drink', 'before', 'bed.']"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dividindo os termos de 'text11' de acordo com um ' '. O resultado é imperfeito.\n",
    "text11 = \"Children shouldn't drink a sugary drink before bed.\"\n",
    "text11.split(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "aaab226d-ae2c-4644-b22d-82bbdd4fbd8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Children',\n",
       " 'should',\n",
       " \"n't\",\n",
       " 'drink',\n",
       " 'a',\n",
       " 'sugary',\n",
       " 'drink',\n",
       " 'before',\n",
       " 'bed',\n",
       " '.']"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# O NLTK não foi apenas capaz de separar 'bed' do '.', como também separou a negação 'n't' de 'should'.\n",
    "nltk.word_tokenize(text11)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db99f54c-8660-46ef-b7a3-12fa3c38bca2",
   "metadata": {},
   "source": [
    "<h3 style='font-size:30px;font-style:italic'>Sentence Splitting</h3>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li> \n",
    "            Dividir os períodos de uma frase pode ser algo bastante complexo para um computador. Isso porque um '.' não indica necessariamente o final de um período.\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "01bf403f-705e-4b82-a5bd-1a8e32ff674c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This is the first sentence',\n",
       " ' A gallon of milk in the U',\n",
       " 'S',\n",
       " ' costs $2',\n",
       " '99',\n",
       " ' Is this the third sentence? Yes, it is!']"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tendo a seguinte frase em mãos, vamos dividí-la com um 'split('.')'.\n",
    "text12 = \"This is the first sentence. A gallon of milk in the U.S. costs $2.99. Is this the third sentence? Yes, it is!\"\n",
    "\n",
    "# O resultado é uma separação bizarra de 'text12'.\n",
    "text12.split('.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "0875733a-7e6c-4097-a8cd-a0bf95da1dab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This is the first sentence.',\n",
       " 'A gallon of milk in the U.S. costs $2.99.',\n",
       " 'Is this the third sentence?',\n",
       " 'Yes, it is!']"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# É considerando essa dificuldade que a nltk possui, também, um separador de frases.\n",
    "nltk.sent_tokenize(text12)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00e94d74-457d-4b39-b38f-03de19fc783e",
   "metadata": {},
   "source": [
    "<div> \n",
    "    <hr>\n",
    "    <h2 style='font-size:30px'>Advanced NLP tasks with NLTK </h2>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "713725b3-9db5-4bc0-a25e-2a4145f05993",
   "metadata": {},
   "source": [
    "<h3 style='font-size:30px;font-style:italic'> Identificação de classes gramaticais</h3>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li> \n",
    "            A Universidade da Pensilvânia criou códigos de identificação para cada classe gramatical da língua inglesa.\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "e9d4f693-e366-41ea-a0a5-f433b3063be0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/veiga/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1041d195-f0dd-400d-ac61-91c5a27539c7",
   "metadata": {},
   "source": [
    "<p style='color:red'> Começar por Advanced NLP tasks with NLTK"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
