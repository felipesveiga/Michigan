{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "357afeaf-8399-49e5-918d-dd69e9237786",
   "metadata": {},
   "source": [
    "<h1 style='font-size:40px'> Classification of Text</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b917510-d898-4592-8861-aad58c04ec27",
   "metadata": {},
   "source": [
    "<h2 style='font-size:30px'> Text Classification</h2>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li> \n",
    "            Classificar um texto pode ser algo de extremo valor. No caso de um website de emails, por exemplo, o Machine Learning é explorado a fim de se identificar emails spam.\n",
    "        </li>\n",
    "        <li> \n",
    "            Da mesma forma que praticamos no terceiro curso da especialização, elaborar um classificador de texto envolve, sempre, uma fase de treino; às vezes, de validação; e, por fim, uma de testes.\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>\n",
    "<center> \n",
    "    <h1> Workflow de ML para NLP</h1>\n",
    "    <img src='ml_workflow.png'>\n",
    "</center>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li> \n",
    "            Assim como tarefas em datasets planilhados, é totalmente concebível construirmos modelos para multi-output classification.\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58da0386-5b31-4a3e-b9b3-94a620afa91c",
   "metadata": {},
   "source": [
    "<div> \n",
    "    <hr>\n",
    "    <h2 style='font-size:30px'> Identifying Features from Text</h2>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3f9828b-c19f-403c-bfd6-23c378e9f331",
   "metadata": {},
   "source": [
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li> \n",
    "            O professor discorre sobre algumas práticas que devem ser levadas em conta quando fazemos trabalhos de NLP.\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>\n",
    "<h3 style='font-size:30px;font-style:italic'> Boas condutas em NLP</h3>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li> \n",
    "            Desconsiderar palavras irrelevantes, como preoposições e conjunções.\n",
    "        </li>\n",
    "        <li> \n",
    "            Lematização e Stemming.\n",
    "        </li>\n",
    "        <li> \n",
    "            Escolher quando a normalização deve ser adequada. Por exemplo, \"US\", de Estados Unidos acaba por ser confundido com o pronome \"us\", quando colocado em minúsculas.\n",
    "        </li>\n",
    "        <li> \n",
    "            Identificação de classes gramaticais.\n",
    "        </li>\n",
    "        <li> \n",
    "            Agrupar termos de mesma categorias, (Mr, Mrs, Phd...), datas.\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07aeffbe-a088-49e4-995a-e4c5dbf41898",
   "metadata": {},
   "source": [
    "<div> \n",
    "    <hr>\n",
    "    <h2 style='font-size:30px'> Naive Bayes Classifiers</h2>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76e043d5-ab5c-4bea-b5d6-9abd2da5683d",
   "metadata": {},
   "source": [
    "<h3 style='font-size:30px;font-style:italic'> Funcionamento.</h3>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li> \n",
    "            Em classificações textuais, temos que oferecer todos os rótulos possíveis que um documento pode receber. De imediato, a probabilidade de cada categoria por si só é computada. Ou seja, as categorias tidas como mais recorrentes receberão uma probabildiade maior a despeito das outras.\n",
    "        </li>\n",
    "        <li> \n",
    "            Com essa etapa feita, o algoritmo estará apto para mensurar as probabilidades de rótulos dado um input (no caso, um texto). \n",
    "        </li>\n",
    "        <li> \n",
    "            Poderíamos montar uma ferramenta de busca que pudesse prever o assunto da query entre \"entretenimento\", \"ciência da computação\" e \"zoologia\", com ela entendendo que a primeira categoria é a mais recorrente. Como ela rotularia uma query \"Python\"?\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c71057e-889c-4263-b674-a42e83585343",
   "metadata": {},
   "source": [
    "<center> \n",
    "    <img src='bayes_prob1.png'>\n",
    "</center>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li> \n",
    "            Neste caso, o algoritmo de bayes multiplicaria as probabilidades-padrão de uma dada categoria à chance de uma consulta pertencente a esse mesmo rótulo ter \"Python\" em seu texto. O produto dessa operação, enfim, é dividido pela chance da palavra \"Python\" se vista, de maneira geral, em qualquer query. Todos esses dados seriam estimados na fase de treino do algoritmo.\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d9640b9-1175-4ef6-aa54-d15c2fd9f049",
   "metadata": {},
   "source": [
    "<h3 style='font-size:30px;font-style:italic'> Suavizando o modelo</h3>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li> \n",
    "            É importante nos atentarmos a casos excepcionais no processo de aprendizado. E se a palavra \"Python\" nunca tivesse aparecido nas consultas de \"Ciência da Computação\" no dataset de treino? Observe que pela fórmula, a probabilidade de esse termo ser designado à categoria mencionada se resultaria em 0. Mas isso seria adequado? \n",
    "        </li>\n",
    "    </ul>\n",
    "</div>\n",
    "<center> \n",
    "    <img src='bayes_smoothing.png'>\n",
    "</center>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li> \n",
    "            Considerando isso, técnicas como o Laplace ou Aditive smoothing foram criadas\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72ec3c91-9cc2-42e9-bf86-bc6cd1f6add9",
   "metadata": {},
   "source": [
    "<center> \n",
    "    <h1> Overview de Naïve Bayes</h1>\n",
    "    <img src='bayes_overview.png'>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae8cd3b1-c753-4fdf-99c1-b8df2c945665",
   "metadata": {},
   "source": [
    "<div> \n",
    "    <hr>\n",
    "    <h2 style='font-size:30px'> Naïve Bayes Variations</h2>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe59534c-eb35-4328-b775-b9c7f5ec83df",
   "metadata": {},
   "source": [
    "<h3 style='font-size:30px;font-style:italic'> Multinomial Naïve Bayes</h3>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li> \n",
    "            O Multinomial Naïve Bayes é um modelo de classificação textual que leva em conta o número de ocorrências de uma palavra.\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>\n",
    "<h3 style='font-size:30px;font-style:italic'> Bernoulli Naïve Bayes</h3>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li> \n",
    "            Já o Bernoulli Naïve Bayes considera apenas a presença ou ausência dos termos no documento.\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15158309-d420-47e6-87b9-d4ea8d5de444",
   "metadata": {},
   "source": [
    "<div> \n",
    "    <hr>\n",
    "    <h2 style='font-size:30px'> Support Vector Machines</h2>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bd56636-c9ba-4303-9569-8faa807621fc",
   "metadata": {},
   "source": [
    "<center> \n",
    "    <h1> Representação gráfica de modelos de NLP</h1>\n",
    "    <img src='svm1.png'>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de0cc6a0-f355-413d-be1a-33c21a2f3926",
   "metadata": {},
   "source": [
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li> \n",
    "            A aula não apresentou nenhuma novidade quanto ao funcionamento das SVM's, mas vale lembrar que o kernel \"linear\" costuma ser bom para tarefas de NLP.\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27435e77-73cd-4ff5-9997-d68f85a4bffe",
   "metadata": {},
   "source": [
    "<div> \n",
    "    <hr>\n",
    "    <h2 style='font-size:30px'>Learning Text Classifiers in Python</h2>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27a52692-eeff-41aa-bda5-4155e7ebbfc7",
   "metadata": {},
   "source": [
    "<p style='color:red'> Learning Text Classifiers in Python (8:35)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
