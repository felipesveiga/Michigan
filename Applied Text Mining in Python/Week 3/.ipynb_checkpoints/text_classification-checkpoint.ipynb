{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "357afeaf-8399-49e5-918d-dd69e9237786",
   "metadata": {},
   "source": [
    "<h1 style='font-size:40px'> Classification of Text</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b917510-d898-4592-8861-aad58c04ec27",
   "metadata": {},
   "source": [
    "<h2 style='font-size:30px'> Text Classification</h2>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li> \n",
    "            Classificar um texto pode ser algo de extremo valor. No caso de um website de emails, por exemplo, o Machine Learning é explorado a fim de se identificar emails spam.\n",
    "        </li>\n",
    "        <li> \n",
    "            Da mesma forma que praticamos no terceiro curso da especialização, elaborar um classificador de texto envolve, sempre, uma fase de treino; às vezes, de validação; e, por fim, uma de testes.\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>\n",
    "<center> \n",
    "    <h1> Workflow de ML para NLP</h1>\n",
    "    <img src='ml_workflow.png'>\n",
    "</center>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li> \n",
    "            Assim como tarefas em datasets planilhados, é totalmente concebível construirmos modelos para multi-output classification.\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58da0386-5b31-4a3e-b9b3-94a620afa91c",
   "metadata": {},
   "source": [
    "<div> \n",
    "    <hr>\n",
    "    <h2 style='font-size:30px'> Identifying Features from Text</h2>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3f9828b-c19f-403c-bfd6-23c378e9f331",
   "metadata": {},
   "source": [
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li> \n",
    "            O professor discorre sobre algumas práticas que devem ser levadas em conta quando fazemos trabalhos de NLP.\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>\n",
    "<h3 style='font-size:30px;font-style:italic'> Boas condutas em NLP</h3>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li> \n",
    "            Desconsiderar palavras irrelevantes, como preoposições e conjunções.\n",
    "        </li>\n",
    "        <li> \n",
    "            Lematização e Stemming.\n",
    "        </li>\n",
    "        <li> \n",
    "            Escolher quando a normalização deve ser adequada. Por exemplo, \"US\", de Estados Unidos acaba por ser confundido com o pronome \"us\", quando colocado em minúsculas.\n",
    "        </li>\n",
    "        <li> \n",
    "            Identificação de classes gramaticais.\n",
    "        </li>\n",
    "        <li> \n",
    "            Agrupar termos de mesma categorias, (Mr, Mrs, Phd...), datas.\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07aeffbe-a088-49e4-995a-e4c5dbf41898",
   "metadata": {},
   "source": [
    "<div> \n",
    "    <hr>\n",
    "    <h2 style='font-size:30px'> Naive Bayes Classifiers</h2>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76e043d5-ab5c-4bea-b5d6-9abd2da5683d",
   "metadata": {},
   "source": [
    "<h3 style='font-size:30px;font-style:italic'> Funcionamento.</h3>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li> \n",
    "            Em classificações textuais, temos que oferecer todos os rótulos possíveis que um documento pode receber. De imediato, a probabilidade de cada categoria por si só é computada. Ou seja, as categorias tidas como mais recorrentes receberão uma probabildiade maior a despeito das outras.\n",
    "        </li>\n",
    "        <li> \n",
    "            Com essa etapa feita, o algoritmo estará apto para mensurar as probabilidades de rótulos dado um input (no caso, um texto). \n",
    "        </li>\n",
    "        <li> \n",
    "            Poderíamos montar uma ferramenta de busca que pudesse prever o assunto da query entre \"entretenimento\", \"ciência da computação\" e \"zoologia\", com ela entendendo que a primeira categoria é a mais recorrente. Como ela rotularia uma query \"Python\"?\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c71057e-889c-4263-b674-a42e83585343",
   "metadata": {},
   "source": [
    "<center> \n",
    "    <img src='bayes_prob1.png'>\n",
    "</center>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li> \n",
    "            Neste caso, o algoritmo de bayes multiplicaria as probabilidades-padrão de uma dada categoria à chance de uma consulta pertencente a esse mesmo rótulo ter \"Python\" em seu texto. O produto dessa operação, enfim, é dividido pela chance da palavra \"Python\" se vista, de maneira geral, em qualquer query. Todos esses dados seriam estimados na fase de treino do algoritmo.\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d9640b9-1175-4ef6-aa54-d15c2fd9f049",
   "metadata": {},
   "source": [
    "<h3 style='font-size:30px;font-style:italic'> Suavizando o modelo</h3>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li> \n",
    "            É importante nos atentarmos a casos excepcionais no processo de aprendizado. E se a palavra \"Python\" nunca tivesse aparecido nas consultas de \"Ciência da Computação\" no dataset de treino? Observe que pela fórmula, a probabilidade de esse termo ser designado à categoria mencionada se resultaria em 0. Mas isso seria adequado? \n",
    "        </li>\n",
    "    </ul>\n",
    "</div>\n",
    "<center> \n",
    "    <img src='bayes_smoothing.png'>\n",
    "</center>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li> \n",
    "            Considerando isso, técnicas como o Laplace ou Aditive smoothing foram criadas\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72ec3c91-9cc2-42e9-bf86-bc6cd1f6add9",
   "metadata": {},
   "source": [
    "<center> \n",
    "    <h1> Overview de Naïve Bayes</h1>\n",
    "    <img src='bayes_overview.png'>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae8cd3b1-c753-4fdf-99c1-b8df2c945665",
   "metadata": {},
   "source": [
    "<div> \n",
    "    <hr>\n",
    "    <h2 style='font-size:30px'> Naïve Bayes Variations</h2>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe59534c-eb35-4328-b775-b9c7f5ec83df",
   "metadata": {},
   "source": [
    "<h3 style='font-size:30px;font-style:italic'> Multinomial Naïve Bayes</h3>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li> \n",
    "            O Multinomial Naïve Bayes é um modelo de classificação textual que leva em conta o número de ocorrências de uma palavra.\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>\n",
    "<h3 style='font-size:30px;font-style:italic'> Bernoulli Naïve Bayes</h3>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li> \n",
    "            Já o Bernoulli Naïve Bayes considera apenas a presença ou ausência dos termos no documento.\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15158309-d420-47e6-87b9-d4ea8d5de444",
   "metadata": {},
   "source": [
    "<div> \n",
    "    <hr>\n",
    "    <h2 style='font-size:30px'> Support Vector Machines</h2>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bd56636-c9ba-4303-9569-8faa807621fc",
   "metadata": {},
   "source": [
    "<center> \n",
    "    <h1> Representação gráfica de modelos de NLP</h1>\n",
    "    <img src='svm1.png'>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27435e77-73cd-4ff5-9997-d68f85a4bffe",
   "metadata": {},
   "source": [
    "<div> \n",
    "    <hr>\n",
    "    <h2 style='font-size:30px'>Learning Text Classifiers in Python</h2>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f4a7cf9-954a-4427-81ab-8782f64c1381",
   "metadata": {},
   "source": [
    "<center> \n",
    "    <h1> Algumas das classes do NLTK para ML</h1>\n",
    "    <img src='nltk_models1.png'>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3831c394-f059-42b8-b901-88d2f59b7810",
   "metadata": {},
   "source": [
    "<center> \n",
    "    <h1> Exemplo de um Naïve Bayes com o NLTK</h1>\n",
    "    <img src='nltk_models2.png'>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97e3f254-056a-413c-8556-ef0bec989f1f",
   "metadata": {},
   "source": [
    "<center> \n",
    "    <h1> Uso de modelos do sklearn com SklearnClassifier</h1>\n",
    "    <img src='nltk_models3.png'>\n",
    "</center>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li> \n",
    "            Passe o modelo desejado como argumento do objeto SklearnClassifier.\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "531a6250-aabe-4c51-97b6-4f54729f0a40",
   "metadata": {},
   "source": [
    "<div> \n",
    "    <hr>\n",
    "    <h2 style='font-size:30px'> Demonstration: Case Study - Sentiment Analysis</h2>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12a030be-cbe4-4ae9-a6ec-48c3862dc6f1",
   "metadata": {},
   "source": [
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li> \n",
    "            Esta aula tem como foco a aplicação dos conceitos aprendidos em uma análise de sentimentos sobre revisões no site da Amazon.\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff506e72-cd13-43d3-b9e0-e884367ee4f1",
   "metadata": {},
   "source": [
    "<h3 style='font-size:30px;font-style:italic'> Importando os dados e limpando-os</h3>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li> \n",
    "            Vamos começar removendo as revisões de 3 estrelas, considerando-as neutras. Além disso, é necessário criar uma coluna target-value.\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "33bea684-d958-403e-83fd-299554295191",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Product Name</th>\n",
       "      <th>Brand Name</th>\n",
       "      <th>Price</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Reviews</th>\n",
       "      <th>Review Votes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...</td>\n",
       "      <td>Samsung</td>\n",
       "      <td>199.99</td>\n",
       "      <td>5</td>\n",
       "      <td>I feel so LUCKY to have found this used (phone...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...</td>\n",
       "      <td>Samsung</td>\n",
       "      <td>199.99</td>\n",
       "      <td>4</td>\n",
       "      <td>nice phone, nice up grade from my pantach revu...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...</td>\n",
       "      <td>Samsung</td>\n",
       "      <td>199.99</td>\n",
       "      <td>5</td>\n",
       "      <td>Very pleased</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...</td>\n",
       "      <td>Samsung</td>\n",
       "      <td>199.99</td>\n",
       "      <td>4</td>\n",
       "      <td>It works good but it goes slow sometimes but i...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...</td>\n",
       "      <td>Samsung</td>\n",
       "      <td>199.99</td>\n",
       "      <td>4</td>\n",
       "      <td>Great phone to replace my lost phone. The only...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        Product Name Brand Name   Price  \\\n",
       "0  \"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...    Samsung  199.99   \n",
       "1  \"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...    Samsung  199.99   \n",
       "2  \"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...    Samsung  199.99   \n",
       "3  \"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...    Samsung  199.99   \n",
       "4  \"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...    Samsung  199.99   \n",
       "\n",
       "   Rating                                            Reviews  Review Votes  \n",
       "0       5  I feel so LUCKY to have found this used (phone...           1.0  \n",
       "1       4  nice phone, nice up grade from my pantach revu...           0.0  \n",
       "2       5                                       Very pleased           0.0  \n",
       "3       4  It works good but it goes slow sometimes but i...           0.0  \n",
       "4       4  Great phone to replace my lost phone. The only...           0.0  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "df = pd.read_csv('Amazon_Unlocked_Mobile.csv').query('Rating!=3')\n",
    "df.dropna(inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e51d37e7-5fe3-4d83-b080-17958323bd1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Product Name</th>\n",
       "      <th>Brand Name</th>\n",
       "      <th>Price</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Reviews</th>\n",
       "      <th>Review Votes</th>\n",
       "      <th>Positively Rated</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...</td>\n",
       "      <td>Samsung</td>\n",
       "      <td>199.99</td>\n",
       "      <td>5</td>\n",
       "      <td>I feel so LUCKY to have found this used (phone...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...</td>\n",
       "      <td>Samsung</td>\n",
       "      <td>199.99</td>\n",
       "      <td>4</td>\n",
       "      <td>nice phone, nice up grade from my pantach revu...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...</td>\n",
       "      <td>Samsung</td>\n",
       "      <td>199.99</td>\n",
       "      <td>5</td>\n",
       "      <td>Very pleased</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...</td>\n",
       "      <td>Samsung</td>\n",
       "      <td>199.99</td>\n",
       "      <td>4</td>\n",
       "      <td>It works good but it goes slow sometimes but i...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...</td>\n",
       "      <td>Samsung</td>\n",
       "      <td>199.99</td>\n",
       "      <td>4</td>\n",
       "      <td>Great phone to replace my lost phone. The only...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        Product Name Brand Name   Price  \\\n",
       "0  \"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...    Samsung  199.99   \n",
       "1  \"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...    Samsung  199.99   \n",
       "2  \"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...    Samsung  199.99   \n",
       "3  \"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...    Samsung  199.99   \n",
       "4  \"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...    Samsung  199.99   \n",
       "\n",
       "   Rating                                            Reviews  Review Votes  \\\n",
       "0       5  I feel so LUCKY to have found this used (phone...           1.0   \n",
       "1       4  nice phone, nice up grade from my pantach revu...           0.0   \n",
       "2       5                                       Very pleased           0.0   \n",
       "3       4  It works good but it goes slow sometimes but i...           0.0   \n",
       "4       4  Great phone to replace my lost phone. The only...           0.0   \n",
       "\n",
       "   Positively Rated  \n",
       "0                 1  \n",
       "1                 1  \n",
       "2                 1  \n",
       "3                 1  \n",
       "4                 1  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Criando a coluna alvo.\n",
    "df['Positively Rated'] = np.where(df.Rating>3, 1, 0)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "500b0a08-8681-4958-9447-647dc3a0185c",
   "metadata": {},
   "source": [
    "<h3 style='font-size:30px;font-style:italic'> train_test_split</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e8593049-ff32-4644-8e4a-251e874c69d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Vamos utilizar apenas a coluna \"Reviews\" como variável independente.\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['Reviews'], df['Positively Rated'], random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57e97eb1-1bd3-4bf0-b004-b760bcafa901",
   "metadata": {},
   "source": [
    "<h3 style='font-size:30px;font-style:italic'> Count Vectorizer</h3>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li> \n",
    "           Aplicar um CountVectorizer sobre os dados textuais é uma das estratégias mais comuns do ML em NLP. Ele cria uma matriz com a contagem de ocorrência de cada token. Vale mencionar que as palavras são automaticamente postas em minúsculas.\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9d8fe8b3-e2e4-4862-bf7f-616b3e41a11a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vect = CountVectorizer().fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e06f881e-38ad-4e18-9bef-50cc5630206e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/veiga/.local/lib/python3.8/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['00',\n",
       " '4less',\n",
       " 'adr6275',\n",
       " 'assignment',\n",
       " 'blazingly',\n",
       " 'cassettes',\n",
       " 'condishion',\n",
       " 'debi',\n",
       " 'dollarsshipping',\n",
       " 'esteem',\n",
       " 'flashy',\n",
       " 'gorila',\n",
       " 'human',\n",
       " 'irullu',\n",
       " 'like',\n",
       " 'microsaudered',\n",
       " 'nightmarish',\n",
       " 'p770',\n",
       " 'poori',\n",
       " 'quirky',\n",
       " 'responseive',\n",
       " 'send',\n",
       " 'sos',\n",
       " 'synch',\n",
       " 'trace',\n",
       " 'utiles',\n",
       " 'withstanding']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Alguns dos tokens de nosso X_train. \n",
    "vect.get_feature_names()[::2000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0c2826ba-fa10-4263-a56b-c52e2d4c9989",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<231207x53216 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 6117776 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Transformando 'X_train' em uma matriz esparsa do scipy.\n",
    "# Cada linha representa o texto de uma review, as colunas indicam cada token do vocabulário.\n",
    "# O valor de cada célula é o número de ocorrências do token em uma determinada review.\n",
    "\n",
    "X_train_vectorized = vect.transform(X_train)\n",
    "X_train_vectorized"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b3d7a05-e7f0-4062-820d-532e10710642",
   "metadata": {},
   "source": [
    "<h3 style='font-size:30px;font-style:italic'> Logistic Regression</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0364921f-e8c3-46b8-9189-1568381485a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/veiga/.local/lib/python3.8/site-packages/sklearn/svm/_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(solver='liblinear')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "model = LogisticRegression(solver='liblinear')\n",
    "model.fit(X_train_vectorized, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6e6efeb4-d267-41c9-bd88-13e0d440d961",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9265439531807432"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Medidno o roc_auc do modelo.\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# Transformando 'X_test'. Note que os tokens que não estavam presentes em 'X_train' serão ignorados.\n",
    "predictions = model.predict(vect.transform(X_test))\n",
    "roc_auc_score(y_test, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c8b717c9-bedb-4d7a-a4df-78b4c0a28b88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Typical Words from bad reviews: ['worst' 'false' 'worthless' 'junk' 'mony' 'garbage' 'useless' 'messing'\n",
      " 'unusable' 'horrible']\n",
      "Typical Words from good reviews: ['lovely' 'amazing' 'perfecto' 'efficient' 'loves' 'loving' 'excellent'\n",
      " 'exelente' 'excelente' 'excelent']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/veiga/.local/lib/python3.8/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "feature_names = np.array(vect.get_feature_names())\n",
    "bottom_top = model.coef_[0].argsort()\n",
    "\n",
    "print(f'Typical Words from bad reviews: {feature_names[bottom_top[:10]]}')\n",
    "print(f'Typical Words from good reviews: {feature_names[bottom_top[-10:]]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "501babc0-9577-4279-a26d-cdf5a1cbb5d9",
   "metadata": {},
   "source": [
    "<h3 style='font-size:30px;font-style:italic'> Tfidf</h3>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li> \n",
    "            O Tdidf Vectorizer é um outro objeto de formação de bag of words com um diferencial importante: ele desconsidera palavras de baixa ocorrência entre os documentos de treino.\n",
    "        </li>\n",
    "        <li> \n",
    "            Utilizar essa classe pode ser uma vantagem sobre o CountVectorizer pela sua menor propensão a overfitting.\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3bd24bfc-1908-4960-add8-c633cfcc1d1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/veiga/.local/lib/python3.8/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "17951"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Definindo que cada palavra deve aparecer em, no mínimo, 5 instâncias de treino diferentes.\n",
    "vect = TfidfVectorizer(min_df=5).fit(X_train)\n",
    "\n",
    "# Note que 'min_df' fez com que o número de features da matriz esparsa caísse.\n",
    "len(vect.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "44128944-141d-476d-b49b-02bf1a013683",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/veiga/.local/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC: 0.9265848398605042\n"
     ]
    }
   ],
   "source": [
    "# Criando a matriz.\n",
    "X_train_vectorized = vect.transform(X_train)\n",
    "\n",
    "# Invocando e treinando o modelo.\n",
    "model = LogisticRegression().fit(X_train_vectorized, y_train)\n",
    "predictions = model.predict(vect.transform(X_test))\n",
    "\n",
    "print(f'AUC: {roc_auc_score(y_test, predictions)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f0b80105-f138-4515-b193-04977198c50f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Uma limitação de nosso modelo é a sua incapacidade de lidar com sequência de palavras. Note que, com um pouco de criatividade,\n",
    "# o enganamos em fazer as suas previsões.\n",
    "\n",
    "# O provável motivo de a primeira frase ser rotulada como negativa é a palavra 'issue'.\n",
    "reviews = ['not an issue, it is working fine', 'no good, don\\'t buy it!']\n",
    "model.predict(vect.transform(reviews))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "875f0c1a-9300-4032-aa58-683baa3beb5c",
   "metadata": {},
   "source": [
    "<h3 style='font-size:30px;font-style:italic'> Tfidf com n-grams</h3>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li> \n",
    "            Esse problema pode ser facilmente solucionado com o uso de n-grams. Se dissermos que, por exemplo, queremos trabalhar com bigrams, o Tfidf Vectorizer considerará sequências de dois token como features, também.\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "009e2e20-2fc0-4717-a749-0c991802da91",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/veiga/.local/lib/python3.8/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "198917"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Agora, pediremos para o modelo considerar tanto sequências de duas palavras, como elas isoladamente.\n",
    "vect = TfidfVectorizer(min_df=5, ngram_range=(1,2)).fit(X_train)\n",
    "\n",
    "# Note que o uso de 'ngram_range' causa uma elevação no número de features, lembrando ao objeto 'PolynomialFeatures'!\n",
    "len(vect.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ce905599-f9e1-4466-8bff-2c15f627be75",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/veiga/.local/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC: 0.9506780240777115\n"
     ]
    }
   ],
   "source": [
    "# Observe que, graças ao uso de n-grams, obtivemos um modelo ainda melhor!\n",
    "X_train_vectorized = vect.transform(X_train)\n",
    "model = LogisticRegression().fit(X_train_vectorized, y_train)\n",
    "predictions = model.predict(vect.transform(X_test))\n",
    "\n",
    "print(f'AUC: {roc_auc_score(y_test, predictions)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dd6b9275-0cca-49ac-8a51-82e72bb1c6b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Problema resolvido!\n",
    "reviews = ['not an issue, it is working fine', 'no good, don\\'t buy it!']\n",
    "model.predict(vect.transform(reviews))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "61ff4047-226e-4add-8380-9cacab6a7655",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Typical terms for bad reviews: ['not' 'disappointed' 'worst' 'poor' 'terrible' 'return' 'horrible'\n",
      " 'doesn' 'not happy' 'slow']\n",
      "Typical terms for good reviews: ['perfectly' 'awesome' 'no problems' 'best' 'not bad' 'amazing' 'perfect'\n",
      " 'excellent' 'love' 'great']\n"
     ]
    }
   ],
   "source": [
    "# Veja que sequências de duas palavras são agora consideradas reviews ruins e boas.\n",
    "feature_names = np.array(vect.get_feature_names())\n",
    "bottom_top = model.coef_[0].argsort()\n",
    "\n",
    "print(f'Typical terms for bad reviews: {feature_names[bottom_top[:10]]}')\n",
    "print(f'Typical terms for good reviews: {feature_names[bottom_top[-10:]]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27a52692-eeff-41aa-bda5-4155e7ebbfc7",
   "metadata": {},
   "source": [
    "<p style='color:red'> Demonstration: Case Study - Sentiment Analysis (4:40)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
